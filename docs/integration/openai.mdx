---
title: "OpenAI"
description: "Use mcp-use tools, resources, and prompts directly with the OpenAI SDK"
icon: "bot"
tag: "New"
---

# Using mcp-use with OpenAI

The OpenAI adapter allows you to seamlessly integrate tools, resources, and prompts from any MCP server with the OpenAI Python SDK. This enables you to use `mcp-use` as a comprehensive tool provider for your OpenAI-powered agents.

## How it Works

The `OpenAIMCPAdapter` converts not only tools but also resources and prompts from your active MCP servers into a format compatible with OpenAI's tool-calling feature. It maps each of these MCP constructs to a callable function that the OpenAI model can request.

-   **Tools** are converted directly to OpenAI functions.
-   **Resources** are converted into functions that take no arguments and read the resource's content.
-   **Prompts** are converted into functions that accept the prompt's arguments.

The adapter maintains a mapping of these generated functions to their actual execution logic, allowing you to easily call them when requested by the model.

## Step-by-Step Guide

Here's how to use the adapter to provide MCP tools, resources, and prompts to an OpenAI Chat Completion.

### Step 1: Initialize MCPClient

First, set up your `MCPClient` with the desired MCP servers. This part of the process is the same as any other `mcp-use` application.

```python
from mcp_use import MCPClient

config = {
    "mcpServers": {
        "airbnb": {"command": "npx", "args": ["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"]},
    }
}

client = MCPClient(config=config)
```

### Step 2: Create the OpenAI Adapter

Next, instantiate the `OpenAIMCPAdapter`. This adapter will be responsible for converting MCP constructs into a format OpenAI can understand.

```python
from mcp_use.adapters import OpenAIMCPAdapter

# Creates the adapter for OpenAI's format
adapter = OpenAIMCPAdapter()
```
<Tip>
You can pass a `disallowed_tools` list to the adapter's constructor to prevent specific tools, resources, or prompts from being exposed to the model.
</Tip>

### Step 3: Generate OpenAI-Compatible Tools

Use the `create_tools` method on the adapter to inspect all connected MCP servers and generate a list of tools in the OpenAI function-calling format.

```python
# Convert tools from active connectors to the OpenAI's format
openai_tools = await adapter.create_tools(client)
```

This list will include functions generated from your MCP tools, resources, and prompts.

### Step 4: Make the Initial API Call

Now, you can use the generated `openai_tools` in a call to the OpenAI API. The model will use the descriptions of these tools to decide if it needs to call any of them to answer the user's query.

```python
from openai import OpenAI

openai = OpenAI()
messages = [
    {"role": "user", "content": "Please tell me the cheapest hotel for two people in Trapani."}
]

response = openai.chat.completions.create(
    model="gpt-4o", 
    messages=messages, 
    tools=openai_tools
)

response_message = response.choices[0].message
messages.append(response_message)
```

### Step 5: Execute Tool Calls

If the model decides to use one or more tools, the `response_message` will contain `tool_calls`. You need to iterate through these calls, execute the corresponding functions, and append the results to your message history.

The `OpenAIMCPAdapter` makes this easy by providing a `tool_executors` dictionary, which maps tool names directly to their async execution functions.

```python
# Handle the tool calls (Tools, Resources, Prompts...)
for tool_call in response_message.tool_calls:
    import json

    function_name = tool_call.function.name
    arguments = json.loads(tool_call.function.arguments)

    # 1. Use the adapter's map to get the correct executor
    executor = adapter.tool_executors.get(function_name)

    if not executor:
        content = f"Error: Tool '{function_name}' not found."
    else:
        try:
            # 2. Execute the tool using the retrieved function
            print(f"Executing tool: {function_name}({arguments})")
            tool_result = await executor(**arguments)

            # 3. Parse the result from any tool type
            if getattr(tool_result, "isError", False):
                content = f"Error: {tool_result.content}"
            elif hasattr(tool_result, "contents"):  # For Resources
                content = "\n".join(c.decode() if isinstance(c, bytes) else str(c) for c in tool_result.contents)
            elif hasattr(tool_result, "messages"):  # For Prompts
                content = "\n".join(str(s) for s in tool_result.messages)
            else:  # For Tools
                content = str(tool_result.content)

        except Exception as e:
            content = f"Error executing tool: {e}"

    # 4. Append the result for this specific tool call
    messages.append(
        {
            "tool_call_id": tool_call.id, 
            "role": "tool", 
            "name": function_name, 
            "content": content
        }
    )
```
This loop correctly handles calling the tool and formatting its output, whether it's from a standard tool, a resource, or a prompt.

### Step 6: Get the Final Response

Finally, send the updated message history which now includes the tool call results back to the model. This allows the model to use the information gathered from the tools to formulate its final answer.

```python
second_response = openai.chat.completions.create(
    model="gpt-4o", 
    messages=messages, 
    tools=openai_tools
)

final_message = second_response.choices[0].message
print("\n--- Final response from the model ---")
print(final_message.content)
```

## Complete Example

For reference, here is the complete, runnable code for integrating mcp-use with the OpenAI SDK.

```python
import asyncio

from dotenv import load_dotenv
from openai import OpenAI

from mcp_use import MCPClient
from mcp_use.adapters import OpenAIMCPAdapter

load_dotenv()

async def main():
    config = {
        "mcpServers": {
            "airbnb": {"command": "npx", "args": ["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"]},
        }
    }

    try:
        client = MCPClient(config=config)
        adapter = OpenAIMCPAdapter()
        openai_tools = await adapter.create_tools(client)
        openai = OpenAI()
        
        messages = [
            {"role": "user", "content": "Please tell me the cheapest hotel for two people in Trapani."}
        ]
        
        response = openai.chat.completions.create(model="gpt-4o", messages=messages, tools=openai_tools)
        response_message = response.choices[0].message
        messages.append(response_message)

        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                import json
                function_name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)
                executor = adapter.tool_executors.get(function_name)

                if not executor:
                    content = f"Error: Tool '{function_name}' not found."
                else:
                    try:
                        tool_result = await executor(**arguments)
                        if getattr(tool_result, "isError", False):
                            content = f"Error: {tool_result.content}"
                        elif hasattr(tool_result, "contents"):
                            content = "\n".join(c.decode() if isinstance(c, bytes) else str(c) for c in tool_result.contents)
                        elif hasattr(tool_result, "messages"):
                            content = "\n".join(str(s) for s in tool_result.messages)
                        else:
                            content = str(tool_result.content)
                    except Exception as e:
                        content = f"Error executing tool: {e}"
                
                messages.append(
                    {
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": function_name,
                        "content": content,
                    }
                )
            
            second_response = openai.chat.completions.create(
                model="gpt-4o", messages=messages, tools=openai_tools
            )
            final_message = second_response.choices[0].message
            print("\n--- Final response from the model ---")
            print(final_message.content)
        else:
            print("No tool call requested by the model")
            print(response_message.content)

    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```