---
title: "Quickstart"
description: "Get started with mcp_use in minutes"
icon: "rocket"
---

# Quickstart Guide

<Info>
This guide will get you started with mcp_use in **under 5 minutes**. We'll cover installation, basic configuration, and running your first agent.
</Info>

## Installation

<CodeGroup>
  ```bash pip
  pip install mcp-use
  ```

  ```bash from source
  git clone https://github.com/mcp-use/mcp-use.git
  cd mcp-use
  pip install -e .
  ```
</CodeGroup>

<Tip>
Installing from source gives you access to the latest features and examples!
</Tip>

## Installing LangChain Providers

mcp_use works with various LLM providers through LangChain. You'll need to install the appropriate LangChain provider package for your chosen LLM:

<CodeGroup>
  ```bash OpenAI
  pip install langchain-openai
  ```

  ```bash Anthropic
  pip install langchain-anthropic
  ```

  ```bash Google
  pip install langchain-google-genai
  ```

  ```bash Groq
  pip install langchain-groq
  ```
</CodeGroup>

<Warning>
**Tool Calling Required**: Only models with tool calling capabilities can be used with mcp_use. Make sure your chosen model supports function calling or tool use.
</Warning>

<Tip>
For other providers, check the [LangChain chat models documentation](https://python.langchain.com/docs/integrations/chat/)
</Tip>

## Environment Setup

<Note>
Set up your environment variables in a `.env` file for secure API key management:
</Note>

```bash .env
OPENAI_API_KEY=your_api_key_here
ANTHROPIC_API_KEY=your_api_key_here
GROQ_API_KEY=your_api_key_here
GOOGLE_API_KEY=your_api_key_here
```

## Your First Agent

Here's a simple example to get you started:

```python
import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    # Load environment variables
    load_dotenv()

    # Create configuration dictionary
    config = {
      "mcpServers": {
        "playwright": {
          "command": "npx",
          "args": ["@playwright/mcp@latest"],
          "env": {
            "DISPLAY": ":1"
          }
        }
      }
    }

    # Create MCPClient from configuration dictionary
    client = MCPClient.from_dict(config)

    # Create LLM
    llm = ChatOpenAI(model="gpt-4o")

    # Create agent with the client
    agent = MCPAgent(llm=llm, client=client, max_steps=30)

    # Run the query
    result = await agent.run(
        "Find the best restaurant in San Francisco USING GOOGLE SEARCH",
    )
    print(f"\nResult: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Configuration Options

You can also add the servers configuration from a config file:

```python
client = MCPClient.from_config_file(
    os.path.join("browser_mcp.json")
)
```

Example configuration file (`browser_mcp.json`):

```json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["@playwright/mcp@latest"],
      "env": {
        "DISPLAY": ":1"
      }
    }
  }
}
```

## Working with Adapters Directly

If you want more control over how tools are created, you can work with the adapters directly. The `BaseAdapter` class provides a unified interface for converting MCP tools to various framework formats, with `LangChainAdapter` being the most commonly used implementation.

```python
import asyncio
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

from mcp_use.client import MCPClient
from mcp_use.adapters import LangChainAdapter

async def main():
    # Initialize client
    client = MCPClient.from_config_file("browser_mcp.json")

    # Create an adapter instance
    adapter = LangChainAdapter()

    # Get tools directly from the client
    tools = await adapter.create_tools(client)

    # Use the tools with any LangChain agent
    llm = ChatOpenAI(model="gpt-4o")
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant with access to powerful tools."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    result = await agent_executor.ainvoke({"input": "Search for information about climate change"})
    print(result["output"])

if __name__ == "__main__":
    asyncio.run(main())
```

The adapter pattern makes it easy to:

1. Create tools directly from an MCPClient
2. Filter or customize which tools are available
3. Integrate with different agent frameworks

## Using Multiple Servers

The `MCPClient` can be configured with multiple MCP servers, allowing your agent to access tools from different sources. This capability enables complex workflows spanning various domains (e.g., web browsing and API interaction).

**Configuration:**

Define multiple servers in your configuration file (`multi_server_config.json`):

```json
{
  "mcpServers": {
    "airbnb": {
      "command": "npx",
      "args": ["-y", "@openbnb/mcp-server-airbnb", "--ignore-robots-txt"]
    },
    "playwright": {
      "command": "npx",
      "args": ["@playwright/mcp@latest"],
      "env": {
        "DISPLAY": ":1"
      }
    }
  }
}
```

**Usage:**

When an `MCPClient` with multiple servers is passed to an `MCPAgent`, the agent gains access to tools from all configured servers. By default, you might need to guide the agent or explicitly specify which server to use for a given task using the `server_name` parameter in the `agent.run()` method.

```python
# Assuming MCPClient is initialized with the multi_server_config.json
client = MCPClient.from_config_file("multi_server_config.json")
agent = MCPAgent(llm=llm, client=client) # Server manager not enabled by default

# Manually specify the server if needed
result = await agent.run(
    "Search for Airbnb listings in Barcelona",
    server_name="airbnb"
)
```

## Enabling Dynamic Server Selection (Server Manager)

To improve efficiency and potentially reduce agent confusion when many tools are available, you can enable the Server Manager. Set `use_server_manager=True` when creating the `MCPAgent`.

When enabled, the agent will automatically select the appropriate server based on the tool chosen by the LLM for each step. This avoids connecting to unnecessary servers.

```python
# Assuming MCPClient is initialized with the multi_server_config.json
client = MCPClient.from_config_file("multi_server_config.json")
agent = MCPAgent(llm=llm, client=client, use_server_manager=True) # Enable server manager

# The agent can now use tools from both airbnb and playwright servers
result = await agent.run(
    "Search for a place in Barcelona on Airbnb, then Google nearby restaurants."
)
```

## Restricting Tool Access

You can control which tools are available to the agent:

```python
import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    # Load environment variables
    load_dotenv()

    # Create configuration dictionary
    config = {
      "mcpServers": {
        "playwright": {
          "command": "npx",
          "args": ["@playwright/mcp@latest"],
          "env": {
            "DISPLAY": ":1"
          }
        }
      }
    }

    # Create MCPClient from configuration dictionary
    client = MCPClient.from_dict(config)

    # Create LLM
    llm = ChatOpenAI(model="gpt-4o")

    # Create agent with restricted tools
    agent = MCPAgent(
        llm=llm,
        client=client,
        max_steps=30,
        disallowed_tools=["file_system", "network"]  # Restrict potentially dangerous tools
    )

    # Run the query
    result = await agent.run(
        "Find the best restaurant in San Francisco USING GOOGLE SEARCH",
    )
    print(f"\nResult: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Available MCP Servers

<CardGroup cols={2}>
  <Card title="Awesome MCP Servers" icon="star" href="https://github.com/punkpeye/awesome-mcp-servers">
    Comprehensive list of available MCP servers from the community
  </Card>
  <Card title="Configuration Guide" icon="gear" href="/essentials/configuration">
    Learn how to configure different types of MCP servers
  </Card>
</CardGroup>

<Info>
mcp_use supports **any MCP server**, allowing you to connect to a wide range of server implementations for different use cases.
</Info>

## Streaming Agent Output

MCP-Use supports asynchronous streaming of agent output using the `astream` method. This allows you to receive incremental results, tool actions, and intermediate steps as they are generated by the agent.

### How to use

Call `agent.astream(query)` and iterate over the results asynchronously:

```python
async for chunk in agent.astream("your query here"):
    print(chunk, end="", flush=True)
```

Internally we use LangChain astream_events API. The output can be found at [How to Stream Runnables](https://python.langchain.com/docs/how_to/streaming/). For now we are using their API,
for consistency (we did not want to reinvent the wheel). Though we admit this is not the easiest version so we are open for suggestions on how you would like the streaming output to look like.
Please submit a request in our github issues [Issues](https://github.com/mcp-use/mcp-use/issues).
We will also post a guide soon on how this can be done in a semi-elegant way with the current set up.


## Next Steps

<CardGroup cols={3}>
  <Card title="Configuration" icon="gear" href="/essentials/configuration">
    Learn advanced configuration options and multi-server setups
  </Card>
  <Card title="LLM Integration" icon="brain" href="/essentials/llm-integration">
    Discover all supported LLM providers and optimization tips
  </Card>
  <Card title="Examples" icon="code" href="https://github.com/mcp-use/mcp-use/tree/main/examples">
    Explore real-world examples and use cases
  </Card>
</CardGroup>

<Tip>
**Need Help?** Join our community discussions on GitHub or check out the comprehensive examples in our repository!
</Tip>
