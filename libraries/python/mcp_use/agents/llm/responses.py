"""This module defines the data structures for representing responses from a Large Language Model."""

from dataclasses import dataclass, field
from typing import Any, Literal


@dataclass
class Function:
    """Represents a function call requested by the model.

    Attributes:
        name: The name of the function to call.
        arguments: The arguments to call the function with, as a JSON string.
    """

    name: str
    arguments: str

    def model_dump(self) -> dict[str, str]:
        """Return a dictionary representation of the Function."""
        return {
            "name": self.name,
            "arguments": self.arguments,
        }


@dataclass
class ToolCall:
    """Represents a tool call requested by the model.

    Attributes:
        type: The type of the tool call, typically 'function'.
        function: The function that was called.
        id: A unique identifier for the tool call.
    """

    type: str
    function: Function
    id: str

    def model_dump(self) -> dict[str, Any]:
        """Return a dictionary representation of the ToolCall."""
        return {
            "type": self.type,
            "function": self.function.model_dump(),
            "id": self.id,
        }


@dataclass
class ToolMessage:
    """Represents the result of a tool call.

    Attributes:
        role: The role of the message, which is 'tool'.
        tool_call_id: The ID of the tool call this message is a response to.
        content: The content of the tool's response.
        name: The name of the tool that was called.
    """

    role: Literal["tool"]
    tool_call_id: str
    content: str
    name: str

    def model_dump(self) -> dict[str, str]:
        """Return a dictionary representation of the ToolMessage."""
        return {
            "role": self.role,
            "tool_call_id": self.tool_call_id,
            "content": self.content,
            "name": self.name,
        }


@dataclass
class Message:
    """Represents a message in the conversation history.

    Attributes:
        role: The role of the entity that produced the message (e.g., 'user', 'assistant').
        content: The text content of the message.
        tool_calls: A list of tool calls requested by the model.
        function_call: A potential function call requested by the model (legacy).
        annotations: A list of annotations for the message.
    """

    role: str
    content: str | None = None
    tool_calls: list[ToolCall] | None = None
    function_call: ToolCall | None = None
    annotations: list[str] | None = None

    def model_dump(self) -> dict[str, Any]:
        """Return a dictionary representation of the Message."""
        dump = {"role": self.role, "content": self.content}
        if self.tool_calls:
            dump["tool_calls"] = [tc.model_dump() for tc in self.tool_calls]
        if self.function_call:
            dump["function_call"] = self.function_call.model_dump()
        if self.annotations:
            dump["annotations"] = self.annotations
        return dump


@dataclass
class Choice:
    """Represents a single choice in an LLM response.

    Attributes:
        finish_reason: The reason the model stopped generating tokens.
        index: The index of the choice in the list of choices.
        message: The message generated by the model.
        provider_specific_fields: A dictionary for any provider-specific data.
    """

    finish_reason: str
    index: int
    message: Message
    provider_specific_fields: dict[str, Any] | None = None

    def model_dump(self) -> dict[str, Any]:
        """Return a dictionary representation of the Choice."""
        return {
            "finish_reason": self.finish_reason,
            "index": self.index,
            "message": self.message.model_dump(),
            "provider_specific_fields": self.provider_specific_fields,
        }


@dataclass
class CompletionTokensDetails:
    """Details about token usage for the completion.

    Attributes:
        accepted_prediction_tokens: Number of accepted prediction tokens.
        audio_tokens: Number of audio tokens.
        reasoning_tokens: Number of reasoning tokens.
        rejected_prediction_tokens: Number of rejected prediction tokens.
        text_tokens: Number of text tokens.
    """

    accepted_prediction_tokens: int | None = None
    audio_tokens: int | None = None
    reasoning_tokens: int | None = None
    rejected_prediction_tokens: int | None = None
    text_tokens: int | None = None


@dataclass
class PromptTokensDetails:
    """Details about token usage for the prompt.

    Attributes:
        audio_tokens: Number of audio tokens.
        cached_tokens: Number of cached tokens.
        text_tokens: Number of text tokens.
        image_tokens: Number of image tokens.
    """

    audio_tokens: int | None = None
    cached_tokens: int | None = None
    text_tokens: int | None = None
    image_tokens: int | None = None


@dataclass
class Usage:
    """Represents the token usage for a request.

    Attributes:
        completion_tokens: The number of tokens in the generated completion.
        prompt_tokens: The number of tokens in the prompt.
        total_tokens: The total number of tokens used in the request.
        completion_tokens_details: Detailed breakdown of completion tokens.
        prompt_tokens_details: Detailed breakdown of prompt tokens.
        service_tier: The service tier for the request.
    """

    completion_tokens: int
    prompt_tokens: int
    total_tokens: int
    completion_tokens_details: CompletionTokensDetails | None = None
    prompt_tokens_details: PromptTokensDetails | None = None
    service_tier: str | None = None


@dataclass
class LLMResponse:
    """Represents the entire response from an LLM.

    Attributes:
        id: A unique identifier for the response.
        created: The timestamp when the response was created.
        model: The model that generated the response.
        object: The type of object, typically 'chat.completion'.
        choices: A list of completion choices.
        usage: Token usage statistics for the request.
        system_fingerprint: A system fingerprint for the response.
    """

    id: str
    created: int
    model: str
    object: str
    choices: list[Choice] = field(default_factory=list)
    usage: Usage | None = None
    system_fingerprint: str | None = None
